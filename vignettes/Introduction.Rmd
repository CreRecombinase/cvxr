---
title: "Introduction to CVXR!"
author: "Stephen Boyd, Steven Diamond, Anqi Fu, Balasubramanian Narasimhan, Paul Rosenfield"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

Welcome to `CVXR`: a modeling language for describing and solving convex optimization problems that follows the natural, mathematical notation of convex optimization rather than the requirements of any particular solver. The purpose of this document is both to introduce the reader to `CVXR` and to generate excitement for its possibilities in the field of statistics.

Convex optimization is a powerful and very general tool. As a practical matter, the set of convex optimization problems includes almost every optimization problem that can be solved exactly and efficiently. If an optimization problem can be solved, it is probably convex. This family of problems becomes even larger if you include those that can be solved _approximately_ and efficiently. While `CVXR` cannot describe every problem in that family, it can describe many of them through a system called Disciplined Convex Programming. (See [dcp.stanford.edu](http://dcp.stanford.edu/) for details.)

The rest of this document presents a sequence of examples, starting from the simple and mundane and building up towards methods of increasing complexity and novelty, including some methods that are state-of-the-art at the time of writing and others that are as-yet-unexplored to our knowledge.

## Hello World

A convex optimization problem has the following form

$$
\begin{aligned}
\text{minimize }& f_0(x) \\
\text{subject to: }& f_i(x) \leq 0, \; i = 1,...,m \\
& g_i(x) = 0, \; i = 1,...,p,
\end{aligned}
$$

where $f_0$ and $f_1,...,f_m$ are convex and $g_1,...,g_p$ are affine. $f_0$ is called the objective function, the $f_i$ are called the inequality constraints, and the $g_i$ are called the equality constraints. We begin with one of the simplest possible problems that presents all three of these features:

```{r, eval=FALSE}
# Variables used to try to minimize the objective
x <- Variable(1)
y <- Variable(1)

# Problem definition
objective <- Minimize(x - y)
constraints <- list(0 <= x + y,
                   x + y <= 1)
prob <- Problem(objective, constraints)

# Problem solution
solution <- solve(prob)
solution$opt.value
solution$x
solution$y

# The world says `hi' back.
```


SHOULD I PROVIDE SOME INVESTIGATION INTO THE CONTENTS OF THE `solution` OBJECT?

```{r, eval = FALSE}
str(solution)
```

## A Canonical Example: Regression

We are exploring the applications of `CVXR` to the field of statistics, so it would be remiss of us not to talk about regression. A lot. We begin by showing what a standard regression problem looks like in `CVXR`.

WHAT IS OUR DATA? We want something in 2 dimensions that works OK with least squares, better with ridge or lasso, even better with huber loss, and maybe even better with some prior knowledge (like how one of the variables has a limited range, and the response is always positive).

One way that Huber would definitely do better is if we corrupt the normal noise term with a slightly fatter normal (basically giving it slightly fatter tails). But that may seem like cheating.


Congratulations! You have just done with `CVXR` what you already could have done with `glmnet`. But slower. And with more code. Stick with us, it gets better.

There is a conspicuous absence from all of these regression problems: constraints! Constraints present a simple way that we can incorporate prior information about the system under study to produce a better prediction.

Add nuance to existing methods by incorporating prior information, or invent entirely new methods with only a few lines of code.







## Stephen Boyd's ideas

here are my ideas (typed in stream of consciousness) for what we should show R users in a user guide, with the script that goes with it.

1.  hello world. form and solve a problem with two variables, one equality and one inequality constraint.

2. simple regression or ridge regression or lasso.  we mention that we can we do this after using glmnet.  (we do this and show the results are close.)  we point out this is *not* what cvx-r is for.


Congratulations! You have just done with cvxr what you could have done with lasso2. But slower. And with more code. Stick with us, it gets better.







3. new we add constraints to the parameters in the ridge regression problem.  you can't do this with glmnet!  we could check the model we find and cross validate and show it's better than the one in (2), maybe.

Question: How would constraining the parameters make the cross-validated error better?



4. now change the loss function in (2) to huber (say).  show an example where we get much better (cross validated) prediction error. (we could then point to a longer set of examples, such as quantile regression, isotonic regression, etc.)



Agriculture example: custom loss function specifically designed to mimic the curve of fertilizer cost vs. yield gain. Or maybe an asymetric penalty function which ascribes higher cost to underpredicting than overpredicting.

Yield vs. fertilizer: steep, then less steep, then flat
cost vs. fertilizer: linear, or maybe including some kinks in it to account for bulk purchases.
Price of corn at time of sale (months into the future) is uncertain.
Objective: maximize expected profit
Other objective: maximize expected profit over 95% confidence interval of corn prices.



For data in which the noise has an unknown distribution, perhaps Min Walsh-average distance regression: robust and perhaps more efficient than L1?



5. more sophisticated examples.  we use rob's new method for including cross terms. (i can give details.)  we can do sparse inverse covariance estimation.  some other ML estimation from exponential family.




6. now we switch to a new problem:   find the maximum entropy distribution on n outcomes that satisfies m expectations (equalities or inequalities).  this is used in, e.g., direct standardization.   we can use it is to estimate a distribution of something from a non-random sample of it, with some known statistics for the whole distribution.

7. if we get cvx-r tied to a solver that does SDP, we can do some great covariance / correlation bounding problems.

8. we can do some cool problems where we maximize/minimize over a set of distributions.


### Example figure with caption

```{r, fig.show='hold', fig.cap='Here\'s a caption!'}
plot(1:10)
plot(10:1)
```

